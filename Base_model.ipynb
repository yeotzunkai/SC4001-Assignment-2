{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlXkQ6qx_Xay"
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mllG7D8s5LxY"
   },
   "source": [
    "# **Steps**\n",
    "\n",
    "Step 1: Load Dataset <p>\n",
    "Step 2: Transform the Dataset <p>\n",
    "Step 3: Create Model <p>\n",
    "Step 4: Train Model <p>\n",
    "Step 5: Save the Model <p>\n",
    "Step 6: Load the Model <p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAe9nBMZ5RPE"
   },
   "source": [
    "# Step 1: Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74CbnuuJAx4Y"
   },
   "outputs": [],
   "source": [
    "data_dir = './dataset'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2M9br4Q5eYA"
   },
   "outputs": [],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsHEhew85fcw"
   },
   "source": [
    "# Step 2: Transform the Dataset\n",
    "\n",
    "The pre-trained networks you'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three sets you'll need to normalize the means and standard deviations of the images to what the network expects. For the means, it's [0.485, 0.456, 0.406] and for the standard deviations [0.229, 0.224, 0.225], calculated from the ImageNet images. These values will shift each color channel to be centered at 0 and range from -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOqT-9GXA8F_"
   },
   "outputs": [],
   "source": [
    "# Define your transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=valid_dir, transform=test_transforms)\n",
    "validloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUtU2o1i5n_U"
   },
   "source": [
    "# Step 3: Create Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSvUud45W61R"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 56 * 56, 512)  # Adjusted this line\n",
    "        self.fc2 = nn.Linear(512, 102)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaZjScXtXU0s"
   },
   "outputs": [],
   "source": [
    "model = CNNModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uGsPXfo6P2A"
   },
   "source": [
    "# Step 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Q2-e-b24PXA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 3.9209, Training Accuracy: 7.69%\n",
      "Validation Loss: 3.6619, Validation Accuracy: 13.69%\n",
      "Epoch 2/100, Training Loss: 3.6439, Training Accuracy: 12.39%\n",
      "Validation Loss: 3.3239, Validation Accuracy: 16.01%\n",
      "Epoch 3/100, Training Loss: 3.3919, Training Accuracy: 15.49%\n",
      "Validation Loss: 3.0524, Validation Accuracy: 23.11%\n",
      "Epoch 4/100, Training Loss: 3.1552, Training Accuracy: 19.57%\n",
      "Validation Loss: 2.8486, Validation Accuracy: 25.67%\n",
      "Epoch 5/100, Training Loss: 3.0101, Training Accuracy: 22.88%\n",
      "Validation Loss: 2.6657, Validation Accuracy: 30.56%\n",
      "Epoch 6/100, Training Loss: 2.8842, Training Accuracy: 25.24%\n",
      "Validation Loss: 2.5906, Validation Accuracy: 32.15%\n",
      "Epoch 7/100, Training Loss: 2.8241, Training Accuracy: 26.79%\n",
      "Validation Loss: 2.4250, Validation Accuracy: 34.72%\n",
      "Epoch 8/100, Training Loss: 2.6907, Training Accuracy: 29.15%\n",
      "Validation Loss: 2.4276, Validation Accuracy: 35.57%\n",
      "Epoch 9/100, Training Loss: 2.6569, Training Accuracy: 30.80%\n",
      "Validation Loss: 2.3125, Validation Accuracy: 39.73%\n",
      "Epoch 10/100, Training Loss: 2.5591, Training Accuracy: 32.25%\n",
      "Validation Loss: 2.2861, Validation Accuracy: 39.61%\n",
      "Epoch 11/100, Training Loss: 2.4938, Training Accuracy: 34.17%\n",
      "Validation Loss: 2.1812, Validation Accuracy: 42.67%\n",
      "Epoch 12/100, Training Loss: 2.4397, Training Accuracy: 34.81%\n",
      "Validation Loss: 2.0890, Validation Accuracy: 44.87%\n",
      "Epoch 13/100, Training Loss: 2.3912, Training Accuracy: 35.71%\n",
      "Validation Loss: 2.0491, Validation Accuracy: 45.48%\n",
      "Epoch 14/100, Training Loss: 2.3333, Training Accuracy: 37.29%\n",
      "Validation Loss: 1.9702, Validation Accuracy: 46.21%\n",
      "Epoch 15/100, Training Loss: 2.2627, Training Accuracy: 39.88%\n",
      "Validation Loss: 1.9574, Validation Accuracy: 48.66%\n",
      "Epoch 16/100, Training Loss: 2.2276, Training Accuracy: 39.84%\n",
      "Validation Loss: 1.8860, Validation Accuracy: 51.10%\n",
      "Epoch 17/100, Training Loss: 2.1865, Training Accuracy: 40.86%\n",
      "Validation Loss: 1.8500, Validation Accuracy: 51.34%\n",
      "Epoch 18/100, Training Loss: 2.0943, Training Accuracy: 42.98%\n",
      "Validation Loss: 1.8342, Validation Accuracy: 52.93%\n",
      "Epoch 19/100, Training Loss: 2.0868, Training Accuracy: 44.26%\n",
      "Validation Loss: 1.8867, Validation Accuracy: 51.71%\n",
      "Epoch 20/100, Training Loss: 2.0636, Training Accuracy: 44.17%\n",
      "Validation Loss: 1.7790, Validation Accuracy: 54.28%\n",
      "Epoch 21/100, Training Loss: 1.9880, Training Accuracy: 46.66%\n",
      "Validation Loss: 1.7635, Validation Accuracy: 56.23%\n",
      "Epoch 22/100, Training Loss: 1.9795, Training Accuracy: 47.04%\n",
      "Validation Loss: 1.7235, Validation Accuracy: 55.01%\n",
      "Epoch 23/100, Training Loss: 1.9508, Training Accuracy: 47.62%\n",
      "Validation Loss: 1.7159, Validation Accuracy: 55.75%\n",
      "Epoch 24/100, Training Loss: 1.9018, Training Accuracy: 47.85%\n",
      "Validation Loss: 1.6081, Validation Accuracy: 58.68%\n",
      "Epoch 25/100, Training Loss: 1.8704, Training Accuracy: 49.44%\n",
      "Validation Loss: 1.6375, Validation Accuracy: 57.95%\n",
      "Epoch 26/100, Training Loss: 1.8560, Training Accuracy: 49.16%\n",
      "Validation Loss: 1.6059, Validation Accuracy: 59.54%\n",
      "Epoch 27/100, Training Loss: 1.7760, Training Accuracy: 51.18%\n",
      "Validation Loss: 1.5401, Validation Accuracy: 60.39%\n",
      "Epoch 28/100, Training Loss: 1.7816, Training Accuracy: 51.86%\n",
      "Validation Loss: 1.5255, Validation Accuracy: 61.74%\n",
      "Epoch 29/100, Training Loss: 1.7718, Training Accuracy: 51.79%\n",
      "Validation Loss: 1.4911, Validation Accuracy: 61.98%\n",
      "Epoch 30/100, Training Loss: 1.7711, Training Accuracy: 52.78%\n",
      "Validation Loss: 1.4605, Validation Accuracy: 63.33%\n",
      "Epoch 31/100, Training Loss: 1.7131, Training Accuracy: 53.74%\n",
      "Validation Loss: 1.4174, Validation Accuracy: 64.43%\n",
      "Epoch 32/100, Training Loss: 1.6616, Training Accuracy: 54.88%\n",
      "Validation Loss: 1.4466, Validation Accuracy: 63.57%\n",
      "Epoch 33/100, Training Loss: 1.6711, Training Accuracy: 54.96%\n",
      "Validation Loss: 1.4524, Validation Accuracy: 62.71%\n",
      "Epoch 34/100, Training Loss: 1.6508, Training Accuracy: 55.49%\n",
      "Validation Loss: 1.4121, Validation Accuracy: 64.55%\n",
      "Epoch 35/100, Training Loss: 1.5879, Training Accuracy: 56.52%\n",
      "Validation Loss: 1.4628, Validation Accuracy: 64.18%\n",
      "Epoch 36/100, Training Loss: 1.5633, Training Accuracy: 57.48%\n",
      "Validation Loss: 1.4190, Validation Accuracy: 64.55%\n",
      "Epoch 37/100, Training Loss: 1.5777, Training Accuracy: 57.01%\n",
      "Validation Loss: 1.4510, Validation Accuracy: 65.53%\n",
      "Epoch 38/100, Training Loss: 1.5227, Training Accuracy: 57.22%\n",
      "Validation Loss: 1.3332, Validation Accuracy: 66.14%\n",
      "Epoch 39/100, Training Loss: 1.5200, Training Accuracy: 57.20%\n",
      "Validation Loss: 1.3305, Validation Accuracy: 66.50%\n",
      "Epoch 40/100, Training Loss: 1.5235, Training Accuracy: 58.26%\n",
      "Validation Loss: 1.3123, Validation Accuracy: 67.36%\n",
      "Epoch 41/100, Training Loss: 1.5127, Training Accuracy: 58.52%\n",
      "Validation Loss: 1.3399, Validation Accuracy: 66.01%\n",
      "Epoch 42/100, Training Loss: 1.4895, Training Accuracy: 58.85%\n",
      "Validation Loss: 1.2913, Validation Accuracy: 68.22%\n",
      "Epoch 43/100, Training Loss: 1.4770, Training Accuracy: 59.25%\n",
      "Validation Loss: 1.2844, Validation Accuracy: 66.87%\n",
      "Epoch 44/100, Training Loss: 1.4439, Training Accuracy: 59.74%\n",
      "Validation Loss: 1.2693, Validation Accuracy: 67.36%\n",
      "Epoch 45/100, Training Loss: 1.4135, Training Accuracy: 61.10%\n",
      "Validation Loss: 1.3436, Validation Accuracy: 66.63%\n",
      "Epoch 46/100, Training Loss: 1.4460, Training Accuracy: 60.36%\n",
      "Validation Loss: 1.2288, Validation Accuracy: 69.44%\n",
      "Epoch 47/100, Training Loss: 1.3957, Training Accuracy: 61.11%\n",
      "Validation Loss: 1.2221, Validation Accuracy: 68.70%\n",
      "Epoch 48/100, Training Loss: 1.4094, Training Accuracy: 60.82%\n",
      "Validation Loss: 1.2275, Validation Accuracy: 68.46%\n",
      "Epoch 49/100, Training Loss: 1.3162, Training Accuracy: 63.13%\n",
      "Validation Loss: 1.2071, Validation Accuracy: 69.19%\n",
      "Epoch 50/100, Training Loss: 1.3485, Training Accuracy: 62.06%\n",
      "Validation Loss: 1.2701, Validation Accuracy: 68.70%\n",
      "Epoch 51/100, Training Loss: 1.3605, Training Accuracy: 61.95%\n",
      "Validation Loss: 1.2396, Validation Accuracy: 68.83%\n",
      "Epoch 52/100, Training Loss: 1.3502, Training Accuracy: 61.63%\n",
      "Validation Loss: 1.1883, Validation Accuracy: 69.19%\n",
      "Epoch 53/100, Training Loss: 1.3029, Training Accuracy: 63.25%\n",
      "Validation Loss: 1.2714, Validation Accuracy: 68.09%\n",
      "Epoch 54/100, Training Loss: 1.3231, Training Accuracy: 62.93%\n",
      "Validation Loss: 1.2339, Validation Accuracy: 68.83%\n",
      "Epoch 55/100, Training Loss: 1.3003, Training Accuracy: 63.22%\n",
      "Validation Loss: 1.2157, Validation Accuracy: 70.17%\n",
      "Epoch 56/100, Training Loss: 1.2674, Training Accuracy: 64.56%\n",
      "Validation Loss: 1.1930, Validation Accuracy: 70.05%\n",
      "Epoch 57/100, Training Loss: 1.2702, Training Accuracy: 64.59%\n",
      "Validation Loss: 1.2158, Validation Accuracy: 71.27%\n",
      "Epoch 58/100, Training Loss: 1.2434, Training Accuracy: 65.84%\n",
      "Validation Loss: 1.1837, Validation Accuracy: 69.80%\n",
      "Epoch 59/100, Training Loss: 1.2516, Training Accuracy: 65.08%\n",
      "Validation Loss: 1.1641, Validation Accuracy: 72.37%\n",
      "Epoch 60/100, Training Loss: 1.2343, Training Accuracy: 65.31%\n",
      "Validation Loss: 1.1489, Validation Accuracy: 70.29%\n",
      "Epoch 61/100, Training Loss: 1.2305, Training Accuracy: 66.36%\n",
      "Validation Loss: 1.2141, Validation Accuracy: 70.29%\n",
      "Epoch 62/100, Training Loss: 1.2636, Training Accuracy: 64.50%\n",
      "Validation Loss: 1.1241, Validation Accuracy: 72.37%\n",
      "Epoch 63/100, Training Loss: 1.2274, Training Accuracy: 65.64%\n",
      "Validation Loss: 1.1179, Validation Accuracy: 70.54%\n",
      "Epoch 64/100, Training Loss: 1.2097, Training Accuracy: 65.83%\n",
      "Validation Loss: 1.1065, Validation Accuracy: 71.27%\n",
      "Epoch 65/100, Training Loss: 1.1696, Training Accuracy: 67.34%\n",
      "Validation Loss: 1.1760, Validation Accuracy: 72.74%\n",
      "Epoch 66/100, Training Loss: 1.2007, Training Accuracy: 66.47%\n",
      "Validation Loss: 1.1663, Validation Accuracy: 70.54%\n",
      "Epoch 67/100, Training Loss: 1.2000, Training Accuracy: 66.04%\n",
      "Validation Loss: 1.0967, Validation Accuracy: 70.54%\n",
      "Epoch 68/100, Training Loss: 1.1714, Training Accuracy: 66.93%\n",
      "Validation Loss: 1.1230, Validation Accuracy: 71.52%\n",
      "Epoch 69/100, Training Loss: 1.1123, Training Accuracy: 67.60%\n",
      "Validation Loss: 1.1077, Validation Accuracy: 71.88%\n",
      "Epoch 70/100, Training Loss: 1.1674, Training Accuracy: 67.87%\n",
      "Validation Loss: 1.1128, Validation Accuracy: 72.13%\n",
      "Epoch 71/100, Training Loss: 1.1232, Training Accuracy: 68.62%\n",
      "Validation Loss: 1.1114, Validation Accuracy: 73.11%\n",
      "Epoch 72/100, Training Loss: 1.1307, Training Accuracy: 68.61%\n",
      "Validation Loss: 1.2035, Validation Accuracy: 70.17%\n",
      "Epoch 73/100, Training Loss: 1.1395, Training Accuracy: 67.74%\n",
      "Validation Loss: 1.1173, Validation Accuracy: 72.37%\n",
      "Epoch 74/100, Training Loss: 1.1434, Training Accuracy: 68.27%\n",
      "Validation Loss: 1.1695, Validation Accuracy: 72.00%\n",
      "Epoch 75/100, Training Loss: 1.1018, Training Accuracy: 69.00%\n",
      "Validation Loss: 1.1447, Validation Accuracy: 72.25%\n",
      "Epoch 76/100, Training Loss: 1.0845, Training Accuracy: 69.35%\n",
      "Validation Loss: 1.1615, Validation Accuracy: 70.90%\n",
      "Epoch 77/100, Training Loss: 1.1308, Training Accuracy: 68.15%\n",
      "Validation Loss: 1.1446, Validation Accuracy: 71.27%\n",
      "Epoch 78/100, Training Loss: 1.0841, Training Accuracy: 69.67%\n",
      "Validation Loss: 1.0590, Validation Accuracy: 73.35%\n",
      "Epoch 79/100, Training Loss: 1.0888, Training Accuracy: 68.76%\n",
      "Validation Loss: 1.1295, Validation Accuracy: 71.88%\n",
      "Epoch 80/100, Training Loss: 1.0922, Training Accuracy: 69.12%\n",
      "Validation Loss: 1.2638, Validation Accuracy: 69.80%\n",
      "Epoch 81/100, Training Loss: 1.1192, Training Accuracy: 69.06%\n",
      "Validation Loss: 1.1080, Validation Accuracy: 72.13%\n",
      "Epoch 82/100, Training Loss: 1.0714, Training Accuracy: 69.63%\n",
      "Validation Loss: 1.0780, Validation Accuracy: 73.72%\n",
      "Epoch 83/100, Training Loss: 1.0671, Training Accuracy: 69.75%\n",
      "Validation Loss: 1.0981, Validation Accuracy: 73.96%\n",
      "Epoch 84/100, Training Loss: 1.0824, Training Accuracy: 69.66%\n",
      "Validation Loss: 1.1473, Validation Accuracy: 72.25%\n",
      "Epoch 85/100, Training Loss: 1.0411, Training Accuracy: 70.53%\n",
      "Validation Loss: 1.0890, Validation Accuracy: 71.76%\n",
      "Epoch 86/100, Training Loss: 1.0536, Training Accuracy: 70.01%\n",
      "Validation Loss: 1.2502, Validation Accuracy: 69.56%\n",
      "Epoch 87/100, Training Loss: 1.0638, Training Accuracy: 69.64%\n",
      "Validation Loss: 1.0597, Validation Accuracy: 71.64%\n",
      "Epoch 88/100, Training Loss: 1.0262, Training Accuracy: 71.34%\n",
      "Validation Loss: 1.1892, Validation Accuracy: 70.42%\n",
      "Epoch 89/100, Training Loss: 1.0218, Training Accuracy: 70.71%\n",
      "Validation Loss: 1.1579, Validation Accuracy: 71.27%\n",
      "Epoch 90/100, Training Loss: 0.9998, Training Accuracy: 71.90%\n",
      "Validation Loss: 1.1336, Validation Accuracy: 72.98%\n",
      "Epoch 91/100, Training Loss: 1.0109, Training Accuracy: 71.29%\n",
      "Validation Loss: 1.1772, Validation Accuracy: 72.74%\n",
      "Epoch 92/100, Training Loss: 1.0385, Training Accuracy: 70.25%\n",
      "Validation Loss: 1.0932, Validation Accuracy: 73.59%\n",
      "Epoch 93/100, Training Loss: 1.0233, Training Accuracy: 70.62%\n",
      "Validation Loss: 1.0627, Validation Accuracy: 73.47%\n",
      "Epoch 94/100, Training Loss: 1.0007, Training Accuracy: 71.50%\n",
      "Validation Loss: 1.1425, Validation Accuracy: 74.33%\n",
      "Epoch 95/100, Training Loss: 0.9674, Training Accuracy: 72.56%\n",
      "Validation Loss: 1.0619, Validation Accuracy: 73.72%\n",
      "Epoch 96/100, Training Loss: 0.9595, Training Accuracy: 72.51%\n",
      "Validation Loss: 1.0665, Validation Accuracy: 75.06%\n",
      "Epoch 97/100, Training Loss: 1.0281, Training Accuracy: 71.20%\n",
      "Validation Loss: 1.1183, Validation Accuracy: 73.11%\n",
      "Epoch 98/100, Training Loss: 0.9814, Training Accuracy: 72.19%\n",
      "Validation Loss: 1.1545, Validation Accuracy: 72.00%\n",
      "Epoch 99/100, Training Loss: 0.9601, Training Accuracy: 72.57%\n",
      "Validation Loss: 1.1157, Validation Accuracy: 73.11%\n",
      "Epoch 100/100, Training Loss: 0.9708, Training Accuracy: 72.74%\n",
      "Validation Loss: 1.0844, Validation Accuracy: 73.35%\n",
      "Training data saved to Results/basedata.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []  # To store training losses\n",
    "train_accuracies = []  # To store training accuracies\n",
    "valid_losses = []  # To store validation losses\n",
    "valid_accuracies = []  # To store validation accuracies\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct_valid = 0\n",
    "    total_valid = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_valid += labels.size(0)\n",
    "            correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * correct_valid / total_valid\n",
    "    valid_losses.append(valid_loss / len(validloader))\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "    print(f\"Validation Loss: {valid_loss / len(validloader):.4f}, Validation Accuracy: {valid_accuracy:.2f}%\")\n",
    "\n",
    "# After the training loop\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Training Loss': train_losses,\n",
    "    'Training Accuracy': train_accuracies,\n",
    "    'Validation Loss': valid_losses,\n",
    "    'Validation Accuracy': valid_accuracies\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file = 'Results/basedata.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f'Training data saved to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 818 test images: 7 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(inputs.shape[0], -1)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        predicted = predicted.view(-1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "       \n",
    "    \n",
    "    print('Accuracy of the network on the %d test images: %d %%' % (total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5mQ52-k6gxP"
   },
   "source": [
    "# Step 5: Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your network is trained, save the model so you can load it later for making predictions. You probably want to save other things such as the mapping of classes to indices which you get from one of the image datasets: image_datasets['train'].class_to_idx. You can attach this to the model as an attribute which makes inference easier later on.\n",
    "\n",
    "model.class_to_idx = image_datasets['train'].class_to_idx\n",
    "\n",
    "Remember that you'll want to completely rebuild the model later so you can use it for inference. Make sure to include any information you need in the checkpoint. If you want to load the model and keep training, you'll want to save the number of epochs as well as the optimizer state, optimizer.state_dict. You'll likely want to use this trained model in the next part of the project, so best to save it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vcTL2RZBdDk"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model,'./base.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjPfeHEZ6mZ1"
   },
   "source": [
    "# Step 6: Load the Model\n",
    "At this point it's good to write a function that can load a checkpoint and rebuild the model. That way you can come back to this project and keep working on it without having to retrain the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbACQoOmN1Dp"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'epoch100.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yeotz\\Documents\\GitHub\\SC4001-Assignment-2\\Base_model.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yeotz/Documents/GitHub/SC4001-Assignment-2/Base_model.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mepoch100.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yeotz/Documents/GitHub/SC4001-Assignment-2/Base_model.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# If you also saved other attributes like optimizer state\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yeotz/Documents/GitHub/SC4001-Assignment-2/Base_model.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mstate_dict()\n",
      "File \u001b[1;32mc:\\Users\\yeotz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\yeotz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\yeotz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'epoch100.pth'"
     ]
    }
   ],
   "source": [
    "model = torch.load('epoch100.pth').to(device)\n",
    "# If you also saved other attributes like optimizer state\n",
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "flower image classifier.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
