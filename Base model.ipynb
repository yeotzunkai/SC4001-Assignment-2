{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlXkQ6qx_Xay"
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mllG7D8s5LxY"
   },
   "source": [
    "# **Steps**\n",
    "\n",
    "Step 1: Load Dataset <p>\n",
    "Step 2: Transform the Dataset <p>\n",
    "Step 3: Create Model <p>\n",
    "Step 4: Train Model <p>\n",
    "Step 5: Save the Model <p>\n",
    "Step 6: Load the Model <p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAe9nBMZ5RPE"
   },
   "source": [
    "# Step 1: Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74CbnuuJAx4Y"
   },
   "outputs": [],
   "source": [
    "data_dir = './dataset'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2M9br4Q5eYA"
   },
   "outputs": [],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsHEhew85fcw"
   },
   "source": [
    "# Step 2: Transform the Dataset\n",
    "\n",
    "The pre-trained networks you'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three sets you'll need to normalize the means and standard deviations of the images to what the network expects. For the means, it's [0.485, 0.456, 0.406] and for the standard deviations [0.229, 0.224, 0.225], calculated from the ImageNet images. These values will shift each color channel to be centered at 0 and range from -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOqT-9GXA8F_"
   },
   "outputs": [],
   "source": [
    "# Define your transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=valid_dir, transform=test_transforms)\n",
    "validloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUtU2o1i5n_U"
   },
   "source": [
    "# Step 3: Create Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSvUud45W61R"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 56 * 56, 512)  # Adjusted this line\n",
    "        self.fc2 = nn.Linear(512, 102)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaZjScXtXU0s"
   },
   "outputs": [],
   "source": [
    "model = CNNModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uGsPXfo6P2A"
   },
   "source": [
    "# Step 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Q2-e-b24PXA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.046\n",
      "[1,     2] loss: 0.228\n",
      "[1,     3] loss: 0.170\n",
      "[1,     4] loss: 0.080\n",
      "[1,     5] loss: 0.049\n",
      "[1,     6] loss: 0.048\n",
      "[1,     7] loss: 0.046\n",
      "[1,     8] loss: 0.046\n",
      "[1,     9] loss: 0.046\n",
      "[1,    10] loss: 0.046\n",
      "[1,    11] loss: 0.046\n",
      "[1,    12] loss: 0.046\n",
      "[1,    13] loss: 0.046\n",
      "[1,    14] loss: 0.045\n",
      "[1,    15] loss: 0.046\n",
      "[1,    16] loss: 0.046\n",
      "[1,    17] loss: 0.046\n",
      "[1,    18] loss: 0.046\n",
      "[1,    19] loss: 0.046\n",
      "[1,    20] loss: 0.046\n",
      "[1,    21] loss: 0.046\n",
      "[1,    22] loss: 0.045\n",
      "[1,    23] loss: 0.046\n",
      "[1,    24] loss: 0.046\n",
      "[1,    25] loss: 0.044\n",
      "[1,    26] loss: 0.046\n",
      "[1,    27] loss: 0.045\n",
      "[1,    28] loss: 0.046\n",
      "[1,    29] loss: 0.045\n",
      "[1,    30] loss: 0.044\n",
      "[1,    31] loss: 0.045\n",
      "[1,    32] loss: 0.044\n",
      "[1,    33] loss: 0.044\n",
      "[1,    34] loss: 0.044\n",
      "[1,    35] loss: 0.045\n",
      "[1,    36] loss: 0.043\n",
      "[1,    37] loss: 0.043\n",
      "[1,    38] loss: 0.045\n",
      "[1,    39] loss: 0.042\n",
      "[1,    40] loss: 0.043\n",
      "[1,    41] loss: 0.043\n",
      "[1,    42] loss: 0.041\n",
      "[1,    43] loss: 0.043\n",
      "[1,    44] loss: 0.041\n",
      "[1,    45] loss: 0.044\n",
      "[1,    46] loss: 0.041\n",
      "[1,    47] loss: 0.042\n",
      "[1,    48] loss: 0.043\n",
      "[1,    49] loss: 0.040\n",
      "[1,    50] loss: 0.040\n",
      "[1,    51] loss: 0.042\n",
      "[1,    52] loss: 0.041\n",
      "[1,    53] loss: 0.042\n",
      "[1,    54] loss: 0.043\n",
      "[1,    55] loss: 0.039\n",
      "[1,    56] loss: 0.041\n",
      "[1,    57] loss: 0.043\n",
      "[1,    58] loss: 0.040\n",
      "[1,    59] loss: 0.041\n",
      "[1,    60] loss: 0.040\n",
      "[1,    61] loss: 0.040\n",
      "[1,    62] loss: 0.041\n",
      "[1,    63] loss: 0.040\n",
      "[1,    64] loss: 0.040\n",
      "[1,    65] loss: 0.042\n",
      "[1,    66] loss: 0.040\n",
      "[1,    67] loss: 0.040\n",
      "[1,    68] loss: 0.039\n",
      "[1,    69] loss: 0.039\n",
      "[1,    70] loss: 0.040\n",
      "[1,    71] loss: 0.038\n",
      "[1,    72] loss: 0.037\n",
      "[1,    73] loss: 0.040\n",
      "[1,    74] loss: 0.038\n",
      "[1,    75] loss: 0.041\n",
      "[1,    76] loss: 0.041\n",
      "[1,    77] loss: 0.040\n",
      "[1,    78] loss: 0.040\n",
      "[1,    79] loss: 0.040\n",
      "[1,    80] loss: 0.039\n",
      "[1,    81] loss: 0.039\n",
      "[1,    82] loss: 0.039\n",
      "[1,    83] loss: 0.040\n",
      "[1,    84] loss: 0.041\n",
      "[1,    85] loss: 0.039\n",
      "[1,    86] loss: 0.039\n",
      "[1,    87] loss: 0.037\n",
      "[1,    88] loss: 0.042\n",
      "[1,    89] loss: 0.036\n",
      "[1,    90] loss: 0.038\n",
      "[1,    91] loss: 0.040\n",
      "[1,    92] loss: 0.039\n",
      "[1,    93] loss: 0.036\n",
      "[1,    94] loss: 0.037\n",
      "[1,    95] loss: 0.040\n",
      "[1,    96] loss: 0.037\n",
      "[1,    97] loss: 0.037\n",
      "[1,    98] loss: 0.038\n",
      "[1,    99] loss: 0.037\n",
      "[1,   100] loss: 0.036\n",
      "[1,   101] loss: 0.040\n",
      "[1,   102] loss: 0.037\n",
      "[1,   103] loss: 0.041\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data[0].to(device),data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(inputs.shape[0], -1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "                (epoch + 1, i + 1, running_loss / 100))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 818 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validloader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(inputs.shape[0], -1)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        predicted = predicted.view(-1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "       \n",
    "    \n",
    "    print('Accuracy of the network on the %d test images: %d %%' % (total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5mQ52-k6gxP"
   },
   "source": [
    "# Step 5: Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your network is trained, save the model so you can load it later for making predictions. You probably want to save other things such as the mapping of classes to indices which you get from one of the image datasets: image_datasets['train'].class_to_idx. You can attach this to the model as an attribute which makes inference easier later on.\n",
    "\n",
    "model.class_to_idx = image_datasets['train'].class_to_idx\n",
    "\n",
    "Remember that you'll want to completely rebuild the model later so you can use it for inference. Make sure to include any information you need in the checkpoint. If you want to load the model and keep training, you'll want to save the number of epochs as well as the optimizer state, optimizer.state_dict. You'll likely want to use this trained model in the next part of the project, so best to save it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vcTL2RZBdDk"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model,'./test.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjPfeHEZ6mZ1"
   },
   "source": [
    "# Step 6: Load the Model\n",
    "At this point it's good to write a function that can load a checkpoint and rebuild the model. That way you can come back to this project and keep working on it without having to retrain the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbACQoOmN1Dp"
   },
   "outputs": [],
   "source": [
    "model = torch.load('epoch100.pth').to(device)\n",
    "# If you also saved other attributes like optimizer state\n",
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "flower image classifier.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
