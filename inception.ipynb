{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = \"./dataset\"\n",
    "train_dir = data_dir + \"/train\"\n",
    "valid_dir = data_dir + \"/valid\"\n",
    "test_dir = data_dir + \"/test\"\n",
    "\n",
    "# Define image transformations\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(299),  # Inception V3 expects 299x299 input\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(320),  # Slightly larger than 299 for cropping\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "train_data = datasets.ImageFolder(\"splitted_dataset/train\", transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(\"splitted_dataset/valid\", transform=valid_transforms)\n",
    "\n",
    "# Using the image datasets and the transforms, define the dataloaders\n",
    "trainloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "validloader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Get class names from the 'class_to_idx' attribute of the dataset\n",
    "class_names = list(train_data.class_to_idx.keys())\n",
    "\n",
    "# Initialize and fit the LabelEncoder with the class names\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(class_names)\n",
    "\n",
    "# Save the fitted LabelEncoder to a file for later use during inference\n",
    "with open(\"label_encoder.pkl\", \"wb\") as file:\n",
    "    pickle.dump(label_encoder, file)\n",
    "\n",
    "\n",
    "# Initialize Inception V3 with pre-trained weights\n",
    "inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in inception.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "# Inception V3's aux_logits=True by default which has an auxiliary output. Generally, it should be turned off by setting to False\n",
    "# when adapting the model to a new task\n",
    "num_ftrs = inception.fc.in_features\n",
    "inception.fc = nn.Linear(num_ftrs, 102)  # Assuming 102 flower classes\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "inception = inception.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(inception.fc.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 5\n",
    "min_val_loss = float(\"inf\")\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    inception.train()  # Set the model to training mode\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = inception(inputs)\n",
    "        # If aux_logits is True, the output will be an InceptionOutputs object\n",
    "        # We need to access the main output for computing the loss\n",
    "        main_outputs = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
    "        loss = criterion(main_outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loop...\n",
    "    valid_loss = 0\n",
    "    accuracy = 0\n",
    "    inception.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = inception(inputs)\n",
    "            # Again, we access the main output for the loss\n",
    "            main_outputs = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
    "            valid_loss += criterion(main_outputs, labels).item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            accuracy += torch.sum(preds == labels.data).item()\n",
    "\n",
    "    # Print out the losses and accuracy\n",
    "    valid_loss = valid_loss / len(validloader.dataset)\n",
    "    accuracy = accuracy / len(validloader.dataset)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "        f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n",
    "        f\"Validation loss: {valid_loss:.3f}.. \"\n",
    "        f\"Validation accuracy: {accuracy:.3f}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping check\n",
    "    if valid_loss < min_val_loss:\n",
    "        min_val_loss = valid_loss\n",
    "        no_improvement_epochs = 0\n",
    "        # Save the model only when validation loss decreases\n",
    "        torch.save(inception.state_dict(), \"inception_flower_model.pth\")\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "        if no_improvement_epochs >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # Scheduler step (after validation)\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the model if not done in the early stopping\n",
    "if no_improvement_epochs < early_stopping_patience:\n",
    "    torch.save(inception.state_dict(), \"inception_flower_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
