{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlXkQ6qx_Xay"
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mllG7D8s5LxY"
   },
   "source": [
    "# **Steps**\n",
    "\n",
    "Step 1: Load Dataset <p>\n",
    "Step 2: Transform the Dataset <p>\n",
    "Step 3: Create Model <p>\n",
    "Step 4: Train Model <p>\n",
    "Step 5: Save the Model <p>\n",
    "Step 6: Load the Model <p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAe9nBMZ5RPE"
   },
   "source": [
    "# Step 1: Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74CbnuuJAx4Y"
   },
   "outputs": [],
   "source": [
    "data_dir = './dataset'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2M9br4Q5eYA"
   },
   "outputs": [],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsHEhew85fcw"
   },
   "source": [
    "# Step 2: Transform the Dataset\n",
    "\n",
    "The pre-trained networks you'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three sets you'll need to normalize the means and standard deviations of the images to what the network expects. For the means, it's [0.485, 0.456, 0.406] and for the standard deviations [0.229, 0.224, 0.225], calculated from the ImageNet images. These values will shift each color channel to be centered at 0 and range from -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOqT-9GXA8F_"
   },
   "outputs": [],
   "source": [
    "# Define your transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=valid_dir, transform=test_transforms)\n",
    "validloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUtU2o1i5n_U"
   },
   "source": [
    "# Step 3: Create Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSvUud45W61R"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class DeformableCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeformableCNNModel, self).__init__()\n",
    "\n",
    "        # Convolutional weights\n",
    "        self.weight1 = nn.Parameter(torch.Tensor(32, 3, 3, 3))\n",
    "        self.weight2 = nn.Parameter(torch.Tensor(64, 32, 3, 3))\n",
    "        self.weight3 = nn.Parameter(torch.Tensor(128, 64, 3, 3))\n",
    "        \n",
    "        # Offset (and mask if required) for deformable convolution\n",
    "        self.offsets1 = nn.Conv2d(3, 2*3*3, kernel_size=3, padding=1, stride=1)\n",
    "        self.offsets2 = nn.Conv2d(32, 2*3*3, kernel_size=3, padding=1, stride=1)\n",
    "        self.offsets3 = nn.Conv2d(64, 2*3*3, kernel_size=3, padding=1, stride=1)\n",
    "        \n",
    "        # Rest of the model remains similar\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 56 * 56, 512)\n",
    "        self.fc2 = nn.Linear(512, 102)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torchvision.ops.deform_conv2d(x, self.offsets1(x), self.weight1, padding=1)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = torchvision.ops.deform_conv2d(x, self.offsets2(x), self.weight2, padding=1)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = torchvision.ops.deform_conv2d(x, self.offsets3(x), self.weight3, padding=1)\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaZjScXtXU0s"
   },
   "outputs": [],
   "source": [
    "model = DeformableCNNModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 102])\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation and dummy input\n",
    "model_test = DeformableCNNModel()\n",
    "input_test = torch.randn(64, 3, 224, 224)  # Random input tensor\n",
    "\n",
    "# Forward pass\n",
    "output_test = model_test(input_test)\n",
    "print(output_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uGsPXfo6P2A"
   },
   "source": [
    "# Step 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Q2-e-b24PXA"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yeotz\\Documents\\GitHub\\SC4001-Assignment-2\\deform_fixed.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yeotz/Documents/GitHub/SC4001-Assignment-2/deform_fixed.ipynb#X50sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yeotz/Documents/GitHub/SC4001-Assignment-2/deform_fixed.ipynb#X50sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yeotz/Documents/GitHub/SC4001-Assignment-2/deform_fixed.ipynb#X50sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yeotz/Documents/GitHub/SC4001-Assignment-2/deform_fixed.ipynb#X50sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Calculate training accuracy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yeotz/Documents/GitHub/SC4001-Assignment-2/deform_fixed.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []  # To store training losses\n",
    "train_accuracies = []  # To store training accuracies\n",
    "valid_losses = []  # To store validation losses\n",
    "valid_accuracies = []  # To store validation accuracies\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(trainloader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct_valid = 0\n",
    "    total_valid = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_valid += labels.size(0)\n",
    "            correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * correct_valid / total_valid\n",
    "    valid_losses.append(valid_loss / len(validloader))\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "    print(f\"Validation Loss: {valid_loss / len(validloader):.4f}, Validation Accuracy: {valid_accuracy:.2f}%\")\n",
    "\n",
    "# After the training loop\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Training Loss': train_losses,\n",
    "    'Training Accuracy': train_accuracies,\n",
    "    'Validation Loss': valid_losses,\n",
    "    'Validation Accuracy': valid_accuracies\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file = 'Results/basedata.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f'Training data saved to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "  for support_set, query_set in validloader:\n",
    "      support_set, query_set = support_set.to(device), query_set.to(device)\n",
    "      embeddings = model(support_set)\n",
    "      _, predicted = torch.max(embeddings, 1)\n",
    "      total += query_set.size(0)\n",
    "      correct += (predicted == query_set).sum().item()\n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40cb23",
   "metadata": {},
   "source": [
    "# Step 5: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f003a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "torch.save(model.state_dict(), 'deform.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d08055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to display an image\n",
    "def imshow(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return ax\n",
    "\n",
    "# Function to make predictions and display images\n",
    "def visualize_predictions(model, dataloader, num_images=5):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    output = model(images)\n",
    "    _, preds = torch.max(output, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "    for i in range(num_images):\n",
    "        imshow(images[i], ax=axes[i], title=f'Predicted: {preds[i]}, Actual: {labels[i]}')\n",
    "\n",
    "# Visualize model predictions on a few validation images\n",
    "visualize_predictions(model, validloader, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a779cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = [p.numel() for p in model_parameters]\n",
    "\n",
    "# Create labels for each layer\n",
    "layer_labels = [f'Layer {i}' for i in range(1, len(params) + 1)]\n",
    "\n",
    "# Create a bar graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(layer_labels, params)\n",
    "plt.xlabel('Model Layers')\n",
    "plt.ylabel('Number of Parameters')\n",
    "plt.title('Number of Parameters in Each Layer')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464694a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate through the validation dataset to collect labels\n",
    "with torch.no_grad():\n",
    "    for images, labels in validloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Convert the confusion matrix to a Pandas DataFrame for visualization\n",
    "cm_df = pd.DataFrame(cm, index=cat_to_name.values(), columns=cat_to_name.values())\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Launch tensorboard using subprocess\n",
    "subprocess.run(['tensorboard', '--logdir', 'runs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62540f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752152f7-c22e-411e-88a0-c6cbd2636870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
